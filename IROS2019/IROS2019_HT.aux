\relax 
\citation{dissanayake2001solution}
\citation{peneda2009trilateration,jung2011indoor}
\citation{peneda2009trilateration,jung2011indoor,raghavan2010accurate}
\citation{newman2003pure,olson2006robust}
\citation{fabresse2018efficient}
\citation{li2017novel}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{\caption@xref {fig:example}{ on input line 112}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison between a standard range-only localization or SLAM framework and our learning-based approach.\relax }}{1}}
\citation{lim2018effective}
\citation{lecun2015deep}
\citation{thomas2005revisiting,cho2010mobile,raghavan2010accurate}
\citation{blanco2008pure,blanco2008efficient,fabresse2013undelayed,shetty2018particle}
\citation{tran2008localization,huan2010three,feng2012determination,afzal2014localization}
\citation{lee2013new,lee2013novel}
\citation{tran2008localization}
\citation{chatterjee2010fletcher,feng2012determination,afzal2014localization}
\citation{samadian2011probabilistic}
\citation{lee2013new,lee2013novel}
\citation{chenna2004state}
\citation{shareef2008localization}
\citation{li2017novel}
\citation{caballero2008particle}
\citation{hochreiter1997long}
\citation{cho2014learning}
\citation{schuster1997bidirectional}
\citation{rahman2009localization,singh2013tdoa,abdelhadi2013efficient,kumar2016localization,banihashemian2018new}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Works}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Conventional RO SLAM}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}LSTM-based Localization}{2}}
\citation{lecun2015deep}
\citation{elman1990finding}
\citation{walch2017image,gladh2016deep,wang2017deepvo,kendall2015posenet,turan2018deep}
\citation{shareef2008localization}
\citation{rahman2009localization}
\citation{singh2013tdoa}
\citation{abdelhadi2013efficient}
\citation{bernas2015fully}
\citation{kumar2016localization}
\citation{banihashemian2018new}
\citation{shareef2008localization,rahman2009localization,singh2013tdoa,abdelhadi2013efficient,bernas2015fully,kumar2016localization,banihashemian2018new}
\citation{rahman2009localization}
\citation{singh2013tdoa}
\citation{abdelhadi2013efficient}
\citation{kumar2016localization}
\citation{banihashemian2018new}
\citation{chatterjee2010fletcher,shareef2008localization,rahman2009localization,singh2013tdoa,banihashemian2018new}
\citation{rahman2009localization}
\citation{shareef2008localization,rahman2009localization,singh2013tdoa,bernas2015fully,kumar2016localization,banihashemian2018new}
\citation{shareef2008localization,rahman2009localization,singh2013tdoa,kumar2016localization}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Deep Learning and Low-dimensional Sensor Data}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}WSN Net}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}long short-Term Memory}{3}}
\citation{schuster1997bidirectional}
\citation{zhang2017multi,li2018human,ullah2018action}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparison of previous studies with our approach\relax }}{4}}
\newlabel{table:related_worsk}{{I}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Architecture of the LSTM. It consists of 3 gates, forget gate(the part inside the red box), input gate(the part inside the blue box), and output gate. And output gate is divided into cell state layer(the part inside the green box) and output gate layer(the part inside the cyan box) \relax }}{4}}
\newlabel{fig:basic_lstm}{{2}{4}}
\newlabel{eq:forget}{{1}{4}}
\newlabel{eq:input}{{2}{4}}
\newlabel{eq:new_cell}{{3}{4}}
\newlabel{eq:update}{{4}{4}}
\newlabel{eq:output}{{5}{4}}
\newlabel{eq:hidden}{{6}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Bidirectional LSTM}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architecture of the Bidirectional LSTM(Bi-LSTM)\relax }}{4}}
\newlabel{fig:bidirectional_revised}{{3}{4}}
\citation{fabresse2013undelayed}
\citation{simonyan2014very,he2016deep}
\citation{graves2013hybrid,graves2013speech,ullah2018action}
\citation{pascanu2013difficulty}
\citation{nair2010rectified}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Stacked Architecture}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-D}Training loss}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Experimental environment}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces our experimental environment briefly and how to the anchor nodes and the tag node are attatched.\relax }}{6}}
\newlabel{fig:anchor_tag_nodes}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Data syncronization for Train/Test data}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Four example of the trajectories\relax }}{6}}
\newlabel{fig:paths}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces the process that makes dataset\relax }}{6}}
\newlabel{fig:dataset}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Training the Networks}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{6}}
\bibstyle{IEEEtran}
\bibdata{./IEEEabrv,./IROS_RObib}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Performance according to the sequence length}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Root mean squared error of each case\relax }}{7}}
\newlabel{table:RMSE_sequence}{{II}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Performance comparison result}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error bar graph of RMSE with respect to sequence length\relax }}{7}}
\newlabel{fig:seq_length}{{7}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}}
\bibcite{dissanayake2001solution}{1}
\bibcite{peneda2009trilateration}{2}
\bibcite{jung2011indoor}{3}
\bibcite{raghavan2010accurate}{4}
\bibcite{newman2003pure}{5}
\bibcite{olson2006robust}{6}
\bibcite{fabresse2018efficient}{7}
\bibcite{li2017novel}{8}
\bibcite{lim2018effective}{9}
\bibcite{lecun2015deep}{10}
\bibcite{thomas2005revisiting}{11}
\bibcite{cho2010mobile}{12}
\bibcite{blanco2008pure}{13}
\bibcite{blanco2008efficient}{14}
\bibcite{fabresse2013undelayed}{15}
\bibcite{shetty2018particle}{16}
\bibcite{tran2008localization}{17}
\bibcite{huan2010three}{18}
\bibcite{feng2012determination}{19}
\bibcite{afzal2014localization}{20}
\bibcite{lee2013new}{21}
\bibcite{lee2013novel}{22}
\bibcite{chatterjee2010fletcher}{23}
\bibcite{samadian2011probabilistic}{24}
\bibcite{chenna2004state}{25}
\bibcite{shareef2008localization}{26}
\bibcite{caballero2008particle}{27}
\bibcite{hochreiter1997long}{28}
\bibcite{cho2014learning}{29}
\bibcite{schuster1997bidirectional}{30}
\bibcite{rahman2009localization}{31}
\bibcite{singh2013tdoa}{32}
\bibcite{abdelhadi2013efficient}{33}
\bibcite{kumar2016localization}{34}
\bibcite{banihashemian2018new}{35}
\bibcite{elman1990finding}{36}
\bibcite{walch2017image}{37}
\bibcite{gladh2016deep}{38}
\bibcite{wang2017deepvo}{39}
\bibcite{kendall2015posenet}{40}
\bibcite{turan2018deep}{41}
\@writefile{toc}{\contentsline {section}{References}{8}}
\bibcite{bernas2015fully}{42}
\bibcite{zhang2017multi}{43}
\bibcite{li2018human}{44}
\bibcite{ullah2018action}{45}
\bibcite{simonyan2014very}{46}
\bibcite{he2016deep}{47}
\bibcite{graves2013hybrid}{48}
\bibcite{graves2013speech}{49}
\bibcite{pascanu2013difficulty}{50}
\bibcite{nair2010rectified}{51}
